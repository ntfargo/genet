{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An file for on-target DeepPE (pre-)training.\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def preprocess_seq(data, seq_length):\n",
    "\n",
    "    seq_onehot = np.zeros((len(data), 1, seq_length, 4), dtype=float)\n",
    "\n",
    "    for l in range(len(data)):\n",
    "        for i in range(seq_length):\n",
    "            try:\n",
    "                data[l][i]\n",
    "            except Exception:\n",
    "                print(data[l], i, seq_length, len(data))\n",
    "\n",
    "            if   data[l][i] in \"Aa\":  seq_onehot[l, 0, i, 0] = 1\n",
    "            elif data[l][i] in \"Cc\":  seq_onehot[l, 0, i, 1] = 1\n",
    "            elif data[l][i] in \"Gg\":  seq_onehot[l, 0, i, 2] = 1\n",
    "            elif data[l][i] in \"Tt\":  seq_onehot[l, 0, i, 3] = 1\n",
    "            elif data[l][i] in \"Xx\":  pass\n",
    "            elif data[l][i] in \"Nn.\": pass\n",
    "            else:\n",
    "                print(\"[Input Error] Non-ATGC character \" + data[l])\n",
    "                sys.exit()\n",
    "\n",
    "    return seq_onehot\n",
    "def seq_concat(data, col1='WT74_On', col2='Edited74_On', seq_length=74):\n",
    "    wt = preprocess_seq(data[col1], seq_length)\n",
    "    ed = preprocess_seq(data[col2], seq_length)\n",
    "    g = np.concatenate((wt, ed), axis=1)\n",
    "    g = 2 * g - 1\n",
    "\n",
    "    return g\n",
    "\n",
    "# LOAD & PREPROCESS GENES\n",
    "\n",
    "data_id = 'DP_variant_293T_PE2_Conv_220428'\n",
    "train_file = pd.read_csv('docs/dataset/%s.csv' % data_id)\n",
    "\n",
    "gene_path = 'docs/dataset/genes/%s.npy' % data_id\n",
    "if not os.path.isfile(gene_path):\n",
    "    g_train = seq_concat(train_file)\n",
    "    np.save(gene_path, g_train)\n",
    "else:\n",
    "    g_train = np.load(gene_path)\n",
    "\n",
    "print(type(g_train))\n",
    "print(g_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_train = torch.tensor(g_train, dtype=torch.float32, device=device)\n",
    "print(type(g_train))\n",
    "print(g_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = g_train.permute((0, 3, 1, 2))\n",
    "print(type(g))\n",
    "print(g.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DeepCas9Model(nn.Module):\n",
    "    def __init__(self, filter_size, filter_num, node_1=80, node_2=60):\n",
    "        super(DeepCas9Model, self).__init__()\n",
    "        length = 30\n",
    "\n",
    "        # Define layers using torch.nn\n",
    "        self.conv1 = nn.Conv2d(4, filter_num[0], kernel_size=(1, filter_size[0]))\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(filter_num[0], filter_num[1], kernel_size=(1, filter_size[1]))\n",
    "        self.norm2 = nn.BatchNorm2d(filter_num[1])\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "\n",
    "        # 원래 DeepSpCas9은 sequential하게 가는 것이 아니긴 한데... 어차피 이미 달라진 모델이니..\n",
    "        # 위에 2d CNN을 2번 거치고, 이후에는 1d CNN\n",
    "\n",
    "        self.conv3 = nn.Conv1d(filter_num[1], filter_num[2], kernel_size=filter_size[2])\n",
    "        self.pool3 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(filter_num[0] * ((length - filter_size[0]) // 2 + 1), node_1)\n",
    "        self.dense2 = nn.Linear(filter_num[1] * ((length - filter_size[1]) // 2 + 1), node_2)\n",
    "        self.output_layer = nn.Linear(filter_num[2] * ((length - filter_size[2]) // 2 + 1), 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.conv1(inputs))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.norm2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = torch.squeeze(x, 2) # dimmension 축소\n",
    "        print('step marker')\n",
    "        x = F.relu(self.conv3(x)) # Error\n",
    "        print('step marker')\n",
    "        x = self.pool3(x)\n",
    "        print('step marker')\n",
    "        x = self.flatten(x)\n",
    "        print('step marker')\n",
    "        x = F.relu(self.dense1(x))\n",
    "        print('step marker')\n",
    "        x = F.relu(self.dense2(x))\n",
    "        print('step marker')\n",
    "        outputs = self.output_layer(x)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def preprocess_seq(data, seq_length):\n",
    "\n",
    "    seq_onehot = np.zeros((len(data), 1, seq_length, 4), dtype=float)\n",
    "\n",
    "    for l in range(len(data)):\n",
    "        for i in range(seq_length):\n",
    "            try:\n",
    "                data[l][i]\n",
    "            except Exception:\n",
    "                print(data[l], i, seq_length, len(data))\n",
    "\n",
    "            if   data[l][i] in \"Aa\":  seq_onehot[l, 0, i, 0] = 1\n",
    "            elif data[l][i] in \"Cc\":  seq_onehot[l, 0, i, 1] = 1\n",
    "            elif data[l][i] in \"Gg\":  seq_onehot[l, 0, i, 2] = 1\n",
    "            elif data[l][i] in \"Tt\":  seq_onehot[l, 0, i, 3] = 1\n",
    "            elif data[l][i] in \"Xx\":  pass\n",
    "            elif data[l][i] in \"Nn.\": pass\n",
    "            else:\n",
    "                print(\"[Input Error] Non-ATGC character \" + data[l])\n",
    "                sys.exit()\n",
    "\n",
    "    return seq_onehot\n",
    "\n",
    "\n",
    "# Create an instance of the custom model\n",
    "filter_size = [3, 5, 7]\n",
    "filter_num  = [100, 70, 40]\n",
    "node_1 = 80\n",
    "node_2 = 60\n",
    "\n",
    "custom_model = DeepCas9Model(filter_size=filter_size, filter_num=filter_num, node_1=node_1, node_2=node_2)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(custom_model.parameters(), lr=0.005)\n",
    "loss_object = nn.MSELoss()\n",
    "\n",
    "# Function for training the model\n",
    "def train_step(inputs, targets):\n",
    "    optimizer.zero_grad()\n",
    "    predictions = custom_model(inputs)\n",
    "    loss = loss_object(predictions, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Function for predicting using the model\n",
    "def predict(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = custom_model(inputs)\n",
    "    return outputs.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class GeneInteractionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size=128, num_layers=1, num_features=24, dropout=0.1):\n",
    "        super(GeneInteractionModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.c1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=128, kernel_size=(1, 3), stride=1, padding=(0, 1)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.c2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=128, out_channels=108, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(108),\n",
    "            nn.GELU(),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv1d(in_channels=108, out_channels=108, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(108),\n",
    "            nn.GELU(),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv1d(in_channels=108, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.r = nn.GRU(128, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.s = nn.Linear(2 * hidden_size, 12, bias=False)\n",
    "\n",
    "        self.d = nn.Sequential(\n",
    "            nn.Linear(num_features, 96, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(96, 64, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 128, bias=False)\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.BatchNorm1d(140),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(140, 1, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, g):\n",
    "        g = torch.squeeze(self.c1(g), 2)\n",
    "        g = self.c2(g)\n",
    "        g, _ = self.r(torch.transpose(g, 1, 2))\n",
    "        g = self.s(g[:, -1, :])\n",
    "        \n",
    "        out = self.head(torch.cat((g), dim=1))\n",
    "\n",
    "        return F.softplus(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('docs/dataset/DeepSpCas9_train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train dataset\n",
    "\n",
    "random_seed = 0\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "x_train = preprocess_seq(df['Target_context'], 30)\n",
    "y_train = df['indel']\n",
    "\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32, device=device)\n",
    "x_train = x_train.permute((0, 3, 1, 2))\n",
    "y_train = torch.tensor(y_train.to_numpy(), dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "\n",
    "# Create an instance of the custom model\n",
    "filter_size = [3, 5, 5]\n",
    "filter_num  = [100, 70, 40]\n",
    "node_1 = 80\n",
    "node_2 = 60\n",
    "\n",
    "model = DeepCas9Model(filter_size=filter_size, filter_num=filter_num, node_1=node_1, node_2=node_2).to(device)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "loss_object = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# Function for training the model\n",
    "def train_step(inputs, targets):\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(inputs)\n",
    "    loss = loss_object(predictions, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "train_ = train_step(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "\n",
    "# Create an instance of the custom model\n",
    "filter_size = [3, 5, 5]\n",
    "filter_num  = [100, 70, 40]\n",
    "node_1 = 80\n",
    "node_2 = 60\n",
    "\n",
    "model = DeepCas9Model(filter_size=filter_size, filter_num=filter_num, node_1=node_1, node_2=node_2).to(device)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "loss_object = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# PARAMS\n",
    "\n",
    "batch_size = 2048\n",
    "learning_rate = 5e-3\n",
    "weight_decay = 5e-2\n",
    "T_0 = 10\n",
    "T_mult = 1\n",
    "hidden_size = 128\n",
    "n_layers = 1\n",
    "n_epochs = 10\n",
    "n_models = 5\n",
    "\n",
    "\n",
    "# TRAINING\n",
    "\n",
    "for m in range(n_models):\n",
    "\n",
    "    random_seed = m\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    model = GeneInteractionModel(hidden_size=hidden_size, num_layers=n_layers).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "\n",
    "    pbar = tqdm(range(n_epochs))\n",
    "    for epoch in pbar:\n",
    "        train_loss = []\n",
    "        train_count = 0\n",
    "\n",
    "        pred = model(x_train)\n",
    "        print(pred.shape)\n",
    "        loss = criterion(pred, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(x.size(0) * loss.detach().cpu().numpy())\n",
    "        train_count += x.size(0)\n",
    "\n",
    "        train_loss = sum(train_loss) / train_count\n",
    "        pbar.set_description('M {:02} | {:.4}'.format(m, train_loss))\n",
    "\n",
    "    torch.save(model.state_dict(),'docs/models/test_model_{}.pt'.format(random_seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
